{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Using Spark Efficiently*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Focus of this notebook is on Spark constructs that can make programs more efficient. \n",
    "In general, this means minimizing the amount of data transfer across nodes, since this is usually the bottleneck for big data analysis problems.*\n",
    "\n",
    "* *Shared variables*\n",
    "    * *Accumulators*\n",
    "    * *Broadcast variables*\n",
    "* *DataFrames*\n",
    "* *Partitioning and the Spark shuffle*\n",
    "* *Piping to external programs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fdd50499780>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spark functions such as __map__ can use variables defined in the driver program, but they make local copies of the variable that are not passed back to the driver program. **Accumulators** are shared variable that allow the aggregation of results from workers back to the driver program, for example, as an event counter. Suppose we want to count the number of rows of data with missing information. The most efficient way is to use an accumulator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " 'The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use',\n",
       " 'it under the terms of the Project Gutenberg License included with this',\n",
       " 'eBook or online at www.gutenberg.org',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses = sc.textFile('/resources/data/Datasets/Ulysses.txt')\n",
    "ulysses.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notice that we have some empty lines. We want to count the number of non-empty lines.\n",
    "num_lines = sc.accumulator(0)\n",
    "#nums_lines = 0\n",
    "\n",
    "def tokenize(line):\n",
    "    remove_punc_map = dict.fromkeys(map(ord, string.punctuation),None)\n",
    "    return line.translate(remove_punc_map).lower().strip().split()\n",
    "    \n",
    "def tokenize_count(line):\n",
    "    global num_lines\n",
    "    print(num_lines)\n",
    "    if line:\n",
    "        num_lines += 1\n",
    "\n",
    "    return tokenize(line)\n",
    "\n",
    "counter = ulysses.flatMap(lambda line: tokenize_count(line)).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=3, value=25302>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25302"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter['circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sometimes we need to send a large read only variable to all workers. For example, we might want to share a large feature matrix to all workers as a part of a machine learning application. This same variable will be sent separately for each parallel operation unless you use a broadcast variable. Also, the default variable passing mechanism is optimized for small variables and can be slow when the variable is large.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets broadcast a variable that holds a dictionary map of all ascii letters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 8, 'd': 3, 't': 19, 'L': 37, 'S': 44, 's': 18, 'K': 36, 'c': 2, 'X': 49, 'V': 47, 'D': 29, 'E': 30, 'R': 43, 'B': 27, 'P': 41, 'z': 25, 'A': 26, 'e': 4, 'o': 14, 'g': 6, 'y': 24, 'I': 34, 'j': 9, 'p': 15, 'F': 31, 'l': 11, 'r': 17, 'q': 16, 'a': 0, 'm': 12, 'Q': 42, 'x': 23, 'Y': 50, 'J': 35, 'f': 5, 'h': 7, 'n': 13, 'O': 40, 'Z': 51, 'M': 38, 'T': 45, 'v': 21, 'k': 10, 'w': 22, 'W': 48, 'u': 20, 'U': 46, 'C': 28, 'G': 32, 'H': 33, 'N': 39, 'b': 1}\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "mapper = dict(zip(string.ascii_letters, count()))\n",
    "print(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets pass the mapper variable to spark context\n",
    "bcv_mapper = sc.broadcast(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2867999"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whereever you wnat to use the broadcast variable, use variable.value\n",
    "#The broadcast variable is sent once to each node and can be re-used many times\n",
    "# lets write a func\n",
    "def weight_first_bc(line, bcv_mapper):\n",
    "    words = tokenize(line)\n",
    "    return sum(bcv_mapper.value.get(word[0], 0) for word in words if word.isalpha())\n",
    "\n",
    "ulysses.map(lambda line: weight_first_bc(line, bcv_mapper)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it looks like \"bcv_mapper\" is being passed to each function, all that is passed is a path to the variable. The worker checks if the path has been cached and uses the cache instead of loading from the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a spark job is submitted, the jobs are divided into stages-> tasks. The tasks actually carries out the execution of the transformations and actions on worker nodes. The drivers's sumbitTask() will serialize the functions and metadata about the broadcast variable to all nodes.\n",
    "\n",
    "__Anatomy of how broadcast works__\n",
    "\n",
    "The Driver creates a local directory to store the data to be broadcasted and launches a HttpServer with access to the directory. The data is actually written into the directory when the broadcast is called (val bdata = sc.broadcast(data)). At the same time, the data is also written into driver's blockManger with a StorageLevel memory + disk. Block manager allocates a blockId (of type BroadcastBlockId) for the data.\n",
    "\n",
    "The real data is broadcasted only when an executor deserializes the task it has received, it also gets the broadcast variable's metadata, in the form of a Broadcast object. It then calls the readObject() method of the metadata object (bdata variable). This method will first check the local block manager to see if there's already a local copy. If not, the data will be fetched from the driver. Once the data is fetched, it's stored in the local block manager for subsequent uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Spark Shuffle and Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Data genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bkxk', 'nxam', 'zauu']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(''.join, np.random.choice(list(string.ascii_lowercase),(,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fake_data(n, val):\n",
    "    users = list(map(''.join, np.random.choice(list(string.ascii_lowercase), (n,2))))\n",
    "    comments = [val]*n\n",
    "    return tuple(zip(users, comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dq', 'lkndnlasdwn'),\n",
       " ('tp', 'lkndnlasdwn'),\n",
       " ('ig', 'lkndnlasdwn'),\n",
       " ('aw', 'lkndnlasdwn'),\n",
       " ('hu', 'lkndnlasdwn'),\n",
       " ('cv', 'lkndnlasdwn'),\n",
       " ('su', 'lkndnlasdwn'),\n",
       " ('vf', 'lkndnlasdwn'),\n",
       " ('bf', 'lkndnlasdwn'),\n",
       " ('vv', 'lkndnlasdwn')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fake_data(10000, 'lkndnlasdwn')\n",
    "list(data)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jv', 'newcomment'),\n",
       " ('bd', 'newcomment'),\n",
       " ('my', 'newcomment'),\n",
       " ('fh', 'newcomment'),\n",
       " ('nj', 'newcomment'),\n",
       " ('kx', 'newcomment'),\n",
       " ('mf', 'newcomment'),\n",
       " ('td', 'newcomment'),\n",
       " ('dj', 'newcomment'),\n",
       " ('uk', 'newcomment')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = fake_data(1000,  'newcomment')\n",
    "list(new_data)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_new = sc.parallelize(new_data).reduceByKey(lambda x, y: x+y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_updated = rdd.join(rdd_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kt',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment')),\n",
       " ('wg',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcommentnewcomment')),\n",
       " ('zw',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcomment'))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_updated.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using partitionBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join operation will hash all the keys of both rdd and rdd_new, sending keys with the same hashes to the same node for the actual join operation. There is a lot of unnecessary data transfer. Since rdd is a much larger data set than rdd_new, we can instead fix the partitioning of rdd and just transfer the keys of rdd_new. This is done by rdd.partitionBy(numPartitions) where numPartitions should be at least twice the number of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize(data).reduceByKey(lambda x, y: x+y)\n",
    "rdd2 = rdd2.partitionBy(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2_updated = rdd2.join(rdd_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zw',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcomment')),\n",
       " ('tp',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment')),\n",
       " ('ey',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcommentnewcomment')),\n",
       " ('uy',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment')),\n",
       " ('yq',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcommentnewcomment')),\n",
       " ('ga',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment')),\n",
       " ('wp',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment')),\n",
       " ('kv',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcomment')),\n",
       " ('pl',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcommentnewcomment')),\n",
       " ('nx',\n",
       "  ('lkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwnlkndnlasdwn',\n",
       "   'newcomment'))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2_updated.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piping to External Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
